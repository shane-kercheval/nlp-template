{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564aa887",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Citation\n",
    "\n",
    "Much of the code and examples are copied/modified from \n",
    "\n",
    "> Blueprints for Text Analytics Using Python by Jens Albrecht, Sidharth Ramachandran, and Christian Winkler (O'Reilly, 2021), 978-1-492-07408-3.\n",
    ">\n",
    "\n",
    "- https://github.com/blueprints-for-text-analytics-python/blueprints-text\n",
    "- https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch08/Topic_Modeling_Clustering.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a967948",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae0c59",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f028ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanekercheval/repos/nlp-template\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66085eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"source/config/notebook_settings.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f360077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3492686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.library.utilities import Timer, get_logger\n",
    "from source.library.text_analysis import count_tokens, tf_idf, get_context_from_keyword, count_keywords, count_keywords_by, impurity\n",
    "from source.library.topic_modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624dedbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started: Loading Data\n",
      "Finished (0.17 seconds)\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"Loading Data\"):\n",
    "    path = 'artifacts/data/processed/un-general-debates-paragraphs.pkl'\n",
    "    paragraphs = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4bfed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055efe1",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "This section provides a basic exploration of the text and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db1a7f",
   "metadata": {},
   "source": [
    "## Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "596aab15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3f95e_row0_col2, #T_3f95e_row0_col4 {\n",
       "  width: 10em;\n",
       "}\n",
       "#T_3f95e_row0_col7 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #B4B7B9 1.0%, transparent 1.0%);\n",
       "}\n",
       "#T_3f95e_row0_col8 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, transparent 50.0%, #B4B7B9 50.0%, #B4B7B9 51.2%, transparent 51.2%);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3f95e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3f95e_level0_col0\" class=\"col_heading level0 col0\" ># of Non-Nulls</th>\n",
       "      <th id=\"T_3f95e_level0_col1\" class=\"col_heading level0 col1\" ># of Nulls</th>\n",
       "      <th id=\"T_3f95e_level0_col2\" class=\"col_heading level0 col2\" >% Nulls</th>\n",
       "      <th id=\"T_3f95e_level0_col3\" class=\"col_heading level0 col3\" ># of Zeros</th>\n",
       "      <th id=\"T_3f95e_level0_col4\" class=\"col_heading level0 col4\" >% Zeros</th>\n",
       "      <th id=\"T_3f95e_level0_col5\" class=\"col_heading level0 col5\" >Mean</th>\n",
       "      <th id=\"T_3f95e_level0_col6\" class=\"col_heading level0 col6\" >St Dev.</th>\n",
       "      <th id=\"T_3f95e_level0_col7\" class=\"col_heading level0 col7\" >Coef of Var</th>\n",
       "      <th id=\"T_3f95e_level0_col8\" class=\"col_heading level0 col8\" >Skewness</th>\n",
       "      <th id=\"T_3f95e_level0_col9\" class=\"col_heading level0 col9\" >Kurtosis</th>\n",
       "      <th id=\"T_3f95e_level0_col10\" class=\"col_heading level0 col10\" >Min</th>\n",
       "      <th id=\"T_3f95e_level0_col11\" class=\"col_heading level0 col11\" >10%</th>\n",
       "      <th id=\"T_3f95e_level0_col12\" class=\"col_heading level0 col12\" >25%</th>\n",
       "      <th id=\"T_3f95e_level0_col13\" class=\"col_heading level0 col13\" >50%</th>\n",
       "      <th id=\"T_3f95e_level0_col14\" class=\"col_heading level0 col14\" >75%</th>\n",
       "      <th id=\"T_3f95e_level0_col15\" class=\"col_heading level0 col15\" >90%</th>\n",
       "      <th id=\"T_3f95e_level0_col16\" class=\"col_heading level0 col16\" >Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3f95e_level0_row0\" class=\"row_heading level0 row0\" >year</th>\n",
       "      <td id=\"T_3f95e_row0_col0\" class=\"data row0 col0\" >279,045</td>\n",
       "      <td id=\"T_3f95e_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_3f95e_row0_col2\" class=\"data row0 col2\" >0.0%</td>\n",
       "      <td id=\"T_3f95e_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_3f95e_row0_col4\" class=\"data row0 col4\" >0.0%</td>\n",
       "      <td id=\"T_3f95e_row0_col5\" class=\"data row0 col5\" >1,992.4</td>\n",
       "      <td id=\"T_3f95e_row0_col6\" class=\"data row0 col6\" >12.6</td>\n",
       "      <td id=\"T_3f95e_row0_col7\" class=\"data row0 col7\" >0.0</td>\n",
       "      <td id=\"T_3f95e_row0_col8\" class=\"data row0 col8\" >0.1</td>\n",
       "      <td id=\"T_3f95e_row0_col9\" class=\"data row0 col9\" >-1.1</td>\n",
       "      <td id=\"T_3f95e_row0_col10\" class=\"data row0 col10\" >1,970</td>\n",
       "      <td id=\"T_3f95e_row0_col11\" class=\"data row0 col11\" >1,975.0</td>\n",
       "      <td id=\"T_3f95e_row0_col12\" class=\"data row0 col12\" >1,982.0</td>\n",
       "      <td id=\"T_3f95e_row0_col13\" class=\"data row0 col13\" >1,993.0</td>\n",
       "      <td id=\"T_3f95e_row0_col14\" class=\"data row0 col14\" >2,003.0</td>\n",
       "      <td id=\"T_3f95e_row0_col15\" class=\"data row0 col15\" >2,010.0</td>\n",
       "      <td id=\"T_3f95e_row0_col16\" class=\"data row0 col16\" >2,015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1080720d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlp.pandas.numeric_summary(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da917720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_536a3_row0_col2, #T_536a3_row1_col2 {\n",
       "  width: 10em;\n",
       "}\n",
       "#T_536a3_row0_col5 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #B4B7B9 0.1%, transparent 0.1%);\n",
       "}\n",
       "#T_536a3_row1_col5 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #B4B7B9 99.9%, transparent 99.9%);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_536a3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_536a3_level0_col0\" class=\"col_heading level0 col0\" ># of Non-Nulls</th>\n",
       "      <th id=\"T_536a3_level0_col1\" class=\"col_heading level0 col1\" ># of Nulls</th>\n",
       "      <th id=\"T_536a3_level0_col2\" class=\"col_heading level0 col2\" >% Nulls</th>\n",
       "      <th id=\"T_536a3_level0_col3\" class=\"col_heading level0 col3\" >Most Freq. Value</th>\n",
       "      <th id=\"T_536a3_level0_col4\" class=\"col_heading level0 col4\" ># of Unique</th>\n",
       "      <th id=\"T_536a3_level0_col5\" class=\"col_heading level0 col5\" >% Unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_536a3_level0_row0\" class=\"row_heading level0 row0\" >country</th>\n",
       "      <td id=\"T_536a3_row0_col0\" class=\"data row0 col0\" >279,045</td>\n",
       "      <td id=\"T_536a3_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_536a3_row0_col2\" class=\"data row0 col2\" >0.0%</td>\n",
       "      <td id=\"T_536a3_row0_col3\" class=\"data row0 col3\" >Russian Federation</td>\n",
       "      <td id=\"T_536a3_row0_col4\" class=\"data row0 col4\" >199</td>\n",
       "      <td id=\"T_536a3_row0_col5\" class=\"data row0 col5\" >0.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_536a3_level0_row1\" class=\"row_heading level0 row1\" >text</th>\n",
       "      <td id=\"T_536a3_row1_col0\" class=\"data row1 col0\" >279,045</td>\n",
       "      <td id=\"T_536a3_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_536a3_row1_col2\" class=\"data row1 col2\" >0.0%</td>\n",
       "      <td id=\"T_536a3_row1_col3\" class=\"data row1 col3\" >The President returned to the [...]</td>\n",
       "      <td id=\"T_536a3_row1_col4\" class=\"data row1 col4\" >278,820</td>\n",
       "      <td id=\"T_536a3_row1_col5\" class=\"data row1 col5\" >99.9%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x107ff0610>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlp.pandas.non_numeric_summary(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed8f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not (paragraphs['text'].str.strip() == '').any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9562c5",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12d367c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6acfcc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords |= {'united', 'nations', 'nation'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b37b6",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e7aedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = paragraphs.sample(2000)\n",
    "#paragraphs.to_pickle('source/tests/test_files/datasets/un_debates_paragraphs_sample.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7380bc51",
   "metadata": {},
   "source": [
    "## TF\n",
    "\n",
    "`TF` seems to be used with `LDA` rather than `TF-IDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7587d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"Calculating TF (uni-grams and bi-grams) for UN Debate Paragraphs\"):\n",
    "    count_vectorizer_unigrams = CountVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "    count_vectors_unigrams = count_vectorizer_unigrams.fit_transform(paragraphs[\"text\"])\n",
    "    count_vectors_unigrams.shape\n",
    "\n",
    "    count_vectorizer_bigrams = CountVectorizer(stop_words=stopwords, ngram_range=(2, 3), min_df=5, max_df=0.7)\n",
    "    count_vectors_bigrams = count_vectorizer_bigrams.fit_transform(paragraphs[\"text\"])\n",
    "    count_vectors_bigrams.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906a83b",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0847ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"Calculating TF-IDF (uni-grams and bi-grams) for UN Debate Paragraphs\"):\n",
    "    tfidf_vectorizer_unigrams = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "    tfidf_vectors_unigrams = tfidf_vectorizer_unigrams.fit_transform(paragraphs[\"text\"])\n",
    "    tfidf_vectors_unigrams.shape\n",
    "    \n",
    "    tfidf_vectorizer_bigrams = TfidfVectorizer(stop_words=stopwords, ngram_range=(2, 3), min_df=5, max_df=0.7)\n",
    "    tfidf_vectors_bigrams = tfidf_vectorizer_bigrams.fit_transform(paragraphs[\"text\"])\n",
    "    tfidf_vectors_bigrams.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bbabfd",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213fe909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    \"\"\"\n",
    "    https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63575ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7911b",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e2e7c",
   "metadata": {},
   "source": [
    "### Uni-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_unigrams = NMF(n_components=10, random_state=42)\n",
    "# see Blueprints pg. 214 for explaination of W X H\n",
    "w_matrix_unigrams = nmf_unigrams.fit_transform(tfidf_vectors_unigrams)\n",
    "h_matrix_unigrams = nmf_unigrams.components_\n",
    "word_names = tfidf_vectorizer_unigrams.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c44df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_matrix_unigrams.shape)\n",
    "print(h_matrix_unigrams.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f5880",
   "metadata": {},
   "source": [
    "Get Topic Weightings for First Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics for first document \n",
    "w_first_doc = w_matrix_unigrams[0, ]\n",
    "w_first_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c928c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be the same values as before\n",
    "predictions_first_doc = nmf_unigrams.transform(tfidf_vectors_unigrams[0,])\n",
    "predictions_first_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[abs(round(x, 4)) for x in (w_first_doc - predictions_first_doc).tolist()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0497a73",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607748d9",
   "metadata": {},
   "source": [
    "Get Top 10 Words for First Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_topic = h_matrix_unigrams[0,]\n",
    "first_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f41392",
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_word_values = first_topic.argsort()[::-1]\n",
    "largest_word_values[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73286faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_names[largest_word_values[0:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7a50d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293defd",
   "metadata": {},
   "source": [
    "Size of Topics (Percent of all Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d703c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_matrix_unigrams.sum(axis=0)/w_matrix_unigrams.sum()*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeecc1ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_to_dictionary(model, features, num_top_words=10):\n",
    "    topics = dict()\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        topics[topic + 1] = [(features[largest[i]], abs(words[largest[i]]*100.0/total)) for i in range(0, num_top_words)]\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79371a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dictionary = topics_to_dictionary(nmf_unigrams, tfidf_vectorizer_unigrams.get_feature_names_out())\n",
    "#topic_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_lookup = {topic:' | '.join([y[0] for y in x[0:3]]) for topic, x in topic_dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_dictionary_to_names(topic_dictionary: dict, num_words_in_name: int=3):\n",
    "    return {topic:' | '.join([y[0] for y in x[0:num_words_in_name]]) for topic, x in topic_dictionary.items()}\n",
    "\n",
    "name_lookup = topic_dictionary_to_names(topic_dictionary, num_words_in_name=2)\n",
    "name_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75acbc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_to_dataframe(model, features: list, num_top_words: int = 10, num_words_in_name: int = 2) -> pd.DataFrame:\n",
    "    topic_dictionary = topics_to_dictionary(model, features, num_top_words)\n",
    "    name_lookup = topic_dictionary_to_names(topic_dictionary, num_words_in_name=num_words_in_name)\n",
    "    \n",
    "    topic_words = pd.DataFrame(topic_dictionary)\n",
    "    topics = topic_words.columns\n",
    "    topic_words = topic_words.reset_index().rename(columns={'index': 'word'})\n",
    "    topic_words = pd.melt(topic_words, id_vars='word', value_vars=list(topics), var_name='topic')\n",
    "    topic_words = topic_words.assign(**pd.DataFrame(topic_words['value'].tolist(), columns=['words', 'value']))\n",
    "    topic_words['label'] = topic_words['topic'].apply(lambda x: name_lookup[x])\n",
    "    return topic_words\n",
    "\n",
    "topic_df = topics_to_dataframe(\n",
    "    model=nmf_unigrams,\n",
    "    features=tfidf_vectorizer_unigrams.get_feature_names_out(),\n",
    "    num_top_words=10,\n",
    "    num_words_in_name=2,\n",
    ")\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a49cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_words = pd.DataFrame(topic_dictionary)\n",
    "# topics = topic_words.columns\n",
    "# topic_words = topic_words.reset_index().rename(columns={'index': 'word'})\n",
    "# topic_words = pd.melt(topic_words, id_vars='word', value_vars=list(topics), var_name='topic')\n",
    "# topic_words = topic_words.assign(**pd.DataFrame(topic_words['value'].tolist(), columns=['words', 'value']))\n",
    "# topic_words['label'] = topic_words['topic'].apply(lambda x: name_lookup[x])\n",
    "# topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad3808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly_express as px\n",
    "\n",
    "fig = px.bar(\n",
    "    topic_df,\n",
    "    x='value',\n",
    "    y='words',\n",
    "    facet_col='label',\n",
    "    facet_col_wrap=3,\n",
    "    facet_col_spacing=0.2,\n",
    "    labels={\n",
    "        'words': '',\n",
    "        'label': '',\n",
    "    },\n",
    "    width=900,\n",
    "    height=1000,\n",
    "    title=\"Topics in NMF model (Unigrams)\"\n",
    ")\n",
    "fig.update_yaxes(matches=None, showticklabels=True, autorange=\"reversed\")\n",
    "#fig.update_xaxes(matches=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a063ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words(\n",
    "    model=nmf_unigrams,\n",
    "    feature_names=tfidf_vectorizer_unigrams.get_feature_names_out(),\n",
    "    n_top_words=5,\n",
    "    title=\"Topics in NMF model (Uni-grams)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ccf33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e025627",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_bigrams = NMF(n_components=10, random_state=42)\n",
    "# see Blueprints pg. 214 for explaination of W X H\n",
    "w_matrix_bigrams = nmf_bigrams.fit_transform(tfidf_vectors_bigrams)\n",
    "h_matrix_bigrams = nmf_bigrams.components_\n",
    "word_names = tfidf_vectorizer_bigrams.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2229e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_topics(nmf_para_model_bigrams, tfidf_para_vectorizer_bigrams.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_df = topics_to_dataframe(\n",
    "    model=nmf_bigrams,\n",
    "    features=tfidf_vectorizer_bigrams.get_feature_names_out(),\n",
    "    num_top_words=10,\n",
    "    num_words_in_name=2,\n",
    ")\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc3cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly_express as px\n",
    "\n",
    "fig = px.bar(\n",
    "    topic_df,\n",
    "    x='value',\n",
    "    y='words',\n",
    "    facet_col='label',\n",
    "    facet_col_wrap=2,\n",
    "    facet_col_spacing=0.2,\n",
    "    labels={\n",
    "        'words': '',\n",
    "        'label': '',\n",
    "    },\n",
    "    width=900,\n",
    "    height=1000,\n",
    "    title=\"Topics in NMF model (Bigrams)\"\n",
    ")\n",
    "fig.update_yaxes(matches=None, showticklabels=True, autorange=\"reversed\")\n",
    "#fig.update_xaxes(matches=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words(\n",
    "    model=nmf_bigrams,\n",
    "    feature_names=tfidf_vectorizer_bigrams.get_feature_names_out(),\n",
    "    n_top_words=5,\n",
    "    title=\"Topics in NMF model (Bi-grams)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce38d4",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a99ab67",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987ee65",
   "metadata": {},
   "source": [
    "Neither the book nor the example above uses TF-IDF with LDA, but do not specify why. Both use TF-IDF with NMF and then change to CountVectorizer with LDA\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/44781047/necessary-to-apply-tf-idf-to-new-documents-in-gensim-lda-model/44789327#44789327\n",
    "\n",
    "> LDA only needs a bag-of-word vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ddb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_para_vectorizer_bigrams = CountVectorizer(stop_words=stopwords, min_df=5, max_df=0.7, ngram_range=(2,3))\n",
    "count_para_vectors_bigrams = count_para_vectorizer_bigrams.fit_transform(paragraphs[\"text\"])\n",
    "count_para_vectors_bigrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_para_vectorizer_bigrams = CountVectorizer(stop_words=stopwords, min_df=5, max_df=0.7, ngram_range=(2,3))\n",
    "count_para_vectors_bigrams = count_para_vectorizer_bigrams.fit_transform(paragraphs[\"text\"])\n",
    "count_para_vectors_bigrams.shape\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_para_vectorizer = CountVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "count_para_vectors = count_para_vectorizer.fit_transform(paragraphs[\"text\"])\n",
    "count_para_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_para_model = LatentDirichletAllocation(n_components = 10, random_state=42)\n",
    "W_lda_para_matrix = lda_para_model.fit_transform(count_para_vectors)\n",
    "H_lda_para_matrix = lda_para_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b6e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words(\n",
    "    model=lda_para_model,\n",
    "    feature_names=count_para_vectorizer.get_feature_names_out(),\n",
    "    n_top_words=5,\n",
    "    title=\"Topics in LDA model (Uni-grams)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4375e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cdf66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_lda_para_matrix.sum(axis=0)/W_lda_para_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a34e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a112e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_para_model_bigrams = LatentDirichletAllocation(n_components = 10, random_state=42)\n",
    "W_lda_para_matrix_bigrams = lda_para_model_bigrams.fit_transform(count_para_vectors_bigrams)\n",
    "H_lda_para_matrix_bigrams = lda_para_model_bigrams.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce47b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    \"\"\"\n",
    "    https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words(\n",
    "    model=lda_para_model_bigrams,\n",
    "    feature_names=count_para_vectorizer_bigrams.get_feature_names_out(),\n",
    "    n_top_words=5,\n",
    "    title=\"Topics in LDA model (Bi-grams)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a56cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f0fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9779e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6aa948",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115d157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ed8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "\n",
    "lda_display = pyLDAvis.sklearn.prepare(lda_para_model, count_para_vectors, count_para_vectorizer, sort_topics=False)\n",
    "#pyLDAvis.display(lda_display)\n",
    "pyLDAvis.save_html(lda_display, 'docs/models/lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe50b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8614b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "\n",
    "lda_display = pyLDAvis.sklearn.prepare(lda_para_model_bigrams, count_para_vectors_bigrams, count_para_vectorizer_bigrams, sort_topics=False)\n",
    "#pyLDAvis.display(lda_display)\n",
    "pyLDAvis.save_html(lda_display, 'docs/models/lda_bigrams.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bee055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5ea39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8c39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
